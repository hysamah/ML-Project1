{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8  #ratio is the percentage of the data allocated for training \n",
    "seed = 1  #random seed for data shuffling\n",
    "x_tr, y_tr, x_te, y_te = preprocess_data() #preprocess intput data from the training and test sets\n",
    "x_tr, x_v, y_tr, y_v = split_data(x_tr, y_tr, ratio, seed) #split training data into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/119: loss=0.1507788120723302\n",
      "GD iter. 1/119: loss=0.1379807832186268\n",
      "GD iter. 2/119: loss=0.12829206477993352\n",
      "GD iter. 3/119: loss=0.12076152783792526\n",
      "GD iter. 4/119: loss=0.11483679099935948\n",
      "GD iter. 5/119: loss=0.11013123064064143\n",
      "GD iter. 6/119: loss=0.10636456660669256\n",
      "GD iter. 7/119: loss=0.10332936490285559\n",
      "GD iter. 8/119: loss=0.10086940201838299\n",
      "GD iter. 9/119: loss=0.09886525901515608\n",
      "GD iter. 10/119: loss=0.09722451219516665\n",
      "GD iter. 11/119: loss=0.09587490310016941\n",
      "GD iter. 12/119: loss=0.09475947724687928\n",
      "GD iter. 13/119: loss=0.09383305117249642\n",
      "GD iter. 14/119: loss=0.09305959544955598\n",
      "GD iter. 15/119: loss=0.09241026334385057\n",
      "GD iter. 16/119: loss=0.09186188432520141\n",
      "GD iter. 17/119: loss=0.09139579892901982\n",
      "GD iter. 18/119: loss=0.09099694874893603\n",
      "GD iter. 19/119: loss=0.09065316006813702\n",
      "GD iter. 20/119: loss=0.09035457637679377\n",
      "GD iter. 21/119: loss=0.09009320659948657\n",
      "GD iter. 22/119: loss=0.08986256403512916\n",
      "GD iter. 23/119: loss=0.08965737690991367\n",
      "GD iter. 24/119: loss=0.08947335577908516\n",
      "GD iter. 25/119: loss=0.08930700625492334\n",
      "GD iter. 26/119: loss=0.08915547799838902\n",
      "GD iter. 27/119: loss=0.08901644280257054\n",
      "GD iter. 28/119: loss=0.08888799606439397\n",
      "GD iter. 29/119: loss=0.08876857709116058\n",
      "GD iter. 30/119: loss=0.08865690459552812\n",
      "GD iter. 31/119: loss=0.08855192445185608\n",
      "GD iter. 32/119: loss=0.08845276735975069\n",
      "GD iter. 33/119: loss=0.08835871451853652\n",
      "GD iter. 34/119: loss=0.0882691697833513\n",
      "GD iter. 35/119: loss=0.0881836370682949\n",
      "GD iter. 36/119: loss=0.08810170199919529\n",
      "GD iter. 37/119: loss=0.08802301700960115\n",
      "GD iter. 38/119: loss=0.08794728922770226\n",
      "GD iter. 39/119: loss=0.0878742706262736\n",
      "GD iter. 40/119: loss=0.08780375000823695\n",
      "GD iter. 41/119: loss=0.08773554648167169\n",
      "GD iter. 42/119: loss=0.0876695041438122\n",
      "GD iter. 43/119: loss=0.08760548774673434\n",
      "GD iter. 44/119: loss=0.08754337916046621\n",
      "GD iter. 45/119: loss=0.08748307448410401\n",
      "GD iter. 46/119: loss=0.08742448168373718\n",
      "GD iter. 47/119: loss=0.08736751865884962\n",
      "GD iter. 48/119: loss=0.08731211165739616\n",
      "GD iter. 49/119: loss=0.08725819397476858\n",
      "GD iter. 50/119: loss=0.08720570488404533\n",
      "GD iter. 51/119: loss=0.0871545887547906\n",
      "GD iter. 52/119: loss=0.08710479432567927\n",
      "GD iter. 53/119: loss=0.08705627410272007\n",
      "GD iter. 54/119: loss=0.08700898386012375\n",
      "GD iter. 55/119: loss=0.08696288222514138\n",
      "GD iter. 56/119: loss=0.08691793033167255\n",
      "GD iter. 57/119: loss=0.08687409153026608\n",
      "GD iter. 58/119: loss=0.08683133114442654\n",
      "GD iter. 59/119: loss=0.08678961626500323\n",
      "GD iter. 60/119: loss=0.08674891557595243\n",
      "GD iter. 61/119: loss=0.08670919920599417\n",
      "GD iter. 62/119: loss=0.0866704386016868\n",
      "GD iter. 63/119: loss=0.0866326064182573\n",
      "GD iter. 64/119: loss=0.08659567642518928\n",
      "GD iter. 65/119: loss=0.08655962342411032\n",
      "GD iter. 66/119: loss=0.08652442317696193\n",
      "GD iter. 67/119: loss=0.08649005234279386\n",
      "GD iter. 68/119: loss=0.08645648842181834\n",
      "GD iter. 69/119: loss=0.08642370970559879\n",
      "GD iter. 70/119: loss=0.08639169523244389\n",
      "GD iter. 71/119: loss=0.0863604247472373\n",
      "GD iter. 72/119: loss=0.0863298786650655\n",
      "GD iter. 73/119: loss=0.08630003803811233\n",
      "GD iter. 74/119: loss=0.08627088452537868\n",
      "GD iter. 75/119: loss=0.0862424003648567\n",
      "GD iter. 76/119: loss=0.086214568347849\n",
      "GD iter. 77/119: loss=0.08618737179517115\n",
      "GD iter. 78/119: loss=0.08616079453501768\n",
      "GD iter. 79/119: loss=0.08613482088230451\n",
      "GD iter. 80/119: loss=0.08610943561932861\n",
      "GD iter. 81/119: loss=0.08608462397760977\n",
      "GD iter. 82/119: loss=0.08606037162079692\n",
      "GD iter. 83/119: loss=0.08603666462853947\n",
      "GD iter. 84/119: loss=0.0860134894812359\n",
      "GD iter. 85/119: loss=0.08599083304558428\n",
      "GD iter. 86/119: loss=0.08596868256086827\n",
      "GD iter. 87/119: loss=0.08594702562592056\n",
      "GD iter. 88/119: loss=0.08592585018671214\n",
      "GD iter. 89/119: loss=0.08590514452452198\n",
      "GD iter. 90/119: loss=0.08588489724464687\n",
      "GD iter. 91/119: loss=0.08586509726561384\n",
      "GD iter. 92/119: loss=0.08584573380886422\n",
      "GD iter. 93/119: loss=0.08582679638887801\n",
      "GD iter. 94/119: loss=0.0858082748037132\n",
      "GD iter. 95/119: loss=0.08579015912593474\n",
      "GD iter. 96/119: loss=0.08577243969391117\n",
      "GD iter. 97/119: loss=0.08575510710345878\n",
      "GD iter. 98/119: loss=0.08573815219981383\n",
      "GD iter. 99/119: loss=0.08572156606991622\n",
      "GD iter. 100/119: loss=0.08570534003498802\n",
      "GD iter. 101/119: loss=0.08568946564339218\n",
      "GD iter. 102/119: loss=0.08567393466375783\n",
      "GD iter. 103/119: loss=0.08565873907835866\n",
      "GD iter. 104/119: loss=0.08564387107673306\n",
      "GD iter. 105/119: loss=0.08562932304953406\n",
      "GD iter. 106/119: loss=0.08561508758259909\n",
      "GD iter. 107/119: loss=0.08560115745122923\n",
      "GD iter. 108/119: loss=0.08558752561466887\n",
      "GD iter. 109/119: loss=0.08557418521077693\n",
      "GD iter. 110/119: loss=0.08556112955088117\n",
      "GD iter. 111/119: loss=0.08554835211480788\n",
      "GD iter. 112/119: loss=0.08553584654608001\n",
      "GD iter. 113/119: loss=0.08552360664727583\n",
      "GD iter. 114/119: loss=0.08551162637554238\n",
      "GD iter. 115/119: loss=0.0854998998382569\n",
      "GD iter. 116/119: loss=0.08548842128883036\n",
      "GD iter. 117/119: loss=0.08547718512264768\n",
      "GD iter. 118/119: loss=0.08546618587313902\n",
      "GD iter. 119/119: loss=0.08545541820797714\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 120\n",
    "gamma = 0.1\n",
    "# Initialization\n",
    "initial_w = generate_w(x_tr.shape)\n",
    "w, loss = mean_squared_error_gd(y_tr, x_tr, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: 0.7416\n",
      "The validation accuracy is: 0.7409600000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is:\", get_accuracy(y_tr, x_tr, w))\n",
    "print(\"The validation accuracy is:\", get_accuracy(y_v, x_v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/99: loss=0.1737371640842426\n",
      "SGD iter. 1/99: loss=0.14669515206981942\n",
      "SGD iter. 2/99: loss=0.13563767608025695\n",
      "SGD iter. 3/99: loss=0.1401534189699989\n",
      "SGD iter. 4/99: loss=0.1733508769874279\n",
      "SGD iter. 5/99: loss=0.12831030189213644\n",
      "SGD iter. 6/99: loss=0.11727087791737931\n",
      "SGD iter. 7/99: loss=0.12904719163620001\n",
      "SGD iter. 8/99: loss=0.12781000145904442\n",
      "SGD iter. 9/99: loss=0.12410732967889447\n",
      "SGD iter. 10/99: loss=0.10781169436733909\n",
      "SGD iter. 11/99: loss=0.12906573234391785\n",
      "SGD iter. 12/99: loss=0.14227563386462994\n",
      "SGD iter. 13/99: loss=0.12428220555223025\n",
      "SGD iter. 14/99: loss=0.100930368071313\n",
      "SGD iter. 15/99: loss=0.11183722571958493\n",
      "SGD iter. 16/99: loss=0.11619715714595874\n",
      "SGD iter. 17/99: loss=0.0978232763698832\n",
      "SGD iter. 18/99: loss=0.11519439589522497\n",
      "SGD iter. 19/99: loss=0.10615784320898063\n",
      "SGD iter. 20/99: loss=0.10337738999435211\n",
      "SGD iter. 21/99: loss=0.10589598129673461\n",
      "SGD iter. 22/99: loss=0.10348563017236682\n",
      "SGD iter. 23/99: loss=0.1001113668908187\n",
      "SGD iter. 24/99: loss=0.1094424898962531\n",
      "SGD iter. 25/99: loss=0.10730151918321057\n",
      "SGD iter. 26/99: loss=0.09480673972566492\n",
      "SGD iter. 27/99: loss=0.10276367671214326\n",
      "SGD iter. 28/99: loss=0.0964209143827561\n",
      "SGD iter. 29/99: loss=0.08950442569737871\n",
      "SGD iter. 30/99: loss=0.09818007516240097\n",
      "SGD iter. 31/99: loss=0.08376586000741006\n",
      "SGD iter. 32/99: loss=0.09911504725518858\n",
      "SGD iter. 33/99: loss=0.09304264864044809\n",
      "SGD iter. 34/99: loss=0.09041945200201595\n",
      "SGD iter. 35/99: loss=0.08767025989489692\n",
      "SGD iter. 36/99: loss=0.087325227086782\n",
      "SGD iter. 37/99: loss=0.08915564299124559\n",
      "SGD iter. 38/99: loss=0.09082514828902281\n",
      "SGD iter. 39/99: loss=0.09209645339347954\n",
      "SGD iter. 40/99: loss=0.08856092266474155\n",
      "SGD iter. 41/99: loss=0.09037959748660394\n",
      "SGD iter. 42/99: loss=0.10879413581370098\n",
      "SGD iter. 43/99: loss=0.08592410800884935\n",
      "SGD iter. 44/99: loss=0.10176294371569095\n",
      "SGD iter. 45/99: loss=0.09383323419028422\n",
      "SGD iter. 46/99: loss=0.08433163204637581\n",
      "SGD iter. 47/99: loss=0.08858838242692946\n",
      "SGD iter. 48/99: loss=0.10567322013096987\n",
      "SGD iter. 49/99: loss=0.09849609193827949\n",
      "SGD iter. 50/99: loss=0.096180458200921\n",
      "SGD iter. 51/99: loss=0.0831127633722907\n",
      "SGD iter. 52/99: loss=0.08152974261551403\n",
      "SGD iter. 53/99: loss=0.10125516740574134\n",
      "SGD iter. 54/99: loss=0.09602793589949718\n",
      "SGD iter. 55/99: loss=0.08538207365025885\n",
      "SGD iter. 56/99: loss=0.09772835349307793\n",
      "SGD iter. 57/99: loss=0.0934317900870763\n",
      "SGD iter. 58/99: loss=0.09023155964023857\n",
      "SGD iter. 59/99: loss=0.07689362986476145\n",
      "SGD iter. 60/99: loss=0.09252464427059343\n",
      "SGD iter. 61/99: loss=0.08731957444970954\n",
      "SGD iter. 62/99: loss=0.08275656419895082\n",
      "SGD iter. 63/99: loss=0.08794554420491921\n",
      "SGD iter. 64/99: loss=0.08746169646341531\n",
      "SGD iter. 65/99: loss=0.09664834310974907\n",
      "SGD iter. 66/99: loss=0.09311794195137586\n",
      "SGD iter. 67/99: loss=0.07878709889676318\n",
      "SGD iter. 68/99: loss=0.09386043638143421\n",
      "SGD iter. 69/99: loss=0.08888698012271373\n",
      "SGD iter. 70/99: loss=0.07840791195065425\n",
      "SGD iter. 71/99: loss=0.08671101719024157\n",
      "SGD iter. 72/99: loss=0.09500625555682353\n",
      "SGD iter. 73/99: loss=0.07969271222799752\n",
      "SGD iter. 74/99: loss=0.07894441958957979\n",
      "SGD iter. 75/99: loss=0.08521240090296998\n",
      "SGD iter. 76/99: loss=0.09654668246445106\n",
      "SGD iter. 77/99: loss=0.08375066831563938\n",
      "SGD iter. 78/99: loss=0.09422142645485869\n",
      "SGD iter. 79/99: loss=0.08312380147174667\n",
      "SGD iter. 80/99: loss=0.09064629550482392\n",
      "SGD iter. 81/99: loss=0.08200156560482594\n",
      "SGD iter. 82/99: loss=0.08801025876950971\n",
      "SGD iter. 83/99: loss=0.08145300194300323\n",
      "SGD iter. 84/99: loss=0.08031527170738303\n",
      "SGD iter. 85/99: loss=0.08726164618855006\n",
      "SGD iter. 86/99: loss=0.0765624940067281\n",
      "SGD iter. 87/99: loss=0.07167091718715987\n",
      "SGD iter. 88/99: loss=0.08068703429360118\n",
      "SGD iter. 89/99: loss=0.09500899240048151\n",
      "SGD iter. 90/99: loss=0.08314335919620099\n",
      "SGD iter. 91/99: loss=0.0934087560072237\n",
      "SGD iter. 92/99: loss=0.08491764240281599\n",
      "SGD iter. 93/99: loss=0.09427095807380773\n",
      "SGD iter. 94/99: loss=0.09210997814894421\n",
      "SGD iter. 95/99: loss=0.07855875001416457\n",
      "SGD iter. 96/99: loss=0.08877453772431596\n",
      "SGD iter. 97/99: loss=0.09331488362531701\n",
      "SGD iter. 98/99: loss=0.08746495352886907\n",
      "SGD iter. 99/99: loss=0.0851446012407488\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.04\n",
    "batch_size = 200\n",
    "seed = 1\n",
    "# Initialization\n",
    "initial_w = generate_w(x_tr.shape)\n",
    "np.random.seed(seed)\n",
    "w, loss = mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iters, gamma, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: 0.7352449999999999\n",
      "The validation accuracy is: 0.7343\n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is:\", get_accuracy(y_tr, x_tr, w))\n",
    "print(\"The validation accuracy is:\", get_accuracy(y_v, x_v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(x_tr, y_tr):\n",
    "    tr_acc = []\n",
    "    vl_acc = []\n",
    "    max_iters = 100\n",
    "    for gamma in np.arange(0.001, 0.1, 0.001):\n",
    "        for batch_size in range(1, x_tr.shape[0],500):\n",
    "            print(\"gamma = \", gamma)\n",
    "            print(\"batches = \", batch_size)\n",
    "\n",
    "            seed = 1\n",
    "            # Initialization\n",
    "            initial_w = generate_w(x_tr.shape)\n",
    "            np.random.seed(seed)\n",
    "            w, loss = mean_squared_error_sgd(y_tr, x_tr, initial_w, max_iters, gamma, batch_size, shuffle = True)\n",
    "            tr_acc.append( get_accuracy(y_tr, x_tr, w))\n",
    "            vl_acc.append(get_accuracy(y_v, x_v, w))\n",
    "            print(\"training accuracy: \", tr_acc[-1])\n",
    "            print(\"validation accuracy: \",vl_acc[-1])\n",
    "    return tr_acc, vl_acc\n",
    "tr, vl = train_sgd(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert remaining test codes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ML\\ML_Project\\ML-Project1\\implementations.py:153: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "[-14.3355906    3.74695612  -9.85496782  -2.18205479   1.87177242\n",
      "  -3.69593298   1.92979028  -3.79830501   4.16413642  -5.40736038\n",
      "  -2.17175116  -6.16257131   5.5696892   -3.72428838   7.71295646\n",
      "  -0.09641613  -0.16425839   0.01823019  -0.041002     0.11286304\n",
      "   0.07848815   0.12541774  -3.21050023  -5.3150292   -0.95675449\n",
      "  -0.85924879  -0.85917859  -3.78157508  -3.73185307  -3.73309048\n",
      "  -4.33145402]\n",
      "The training accuracy is: -28.02765\n",
      "The validation accuracy is: -27.96252\n"
     ]
    }
   ],
   "source": [
    "initial_w = generate_w(x_tr.shape)\n",
    "max_iters = 10\n",
    "gamma = 0.0001\n",
    "w, loss = logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "print(loss)\n",
    "print(w)\n",
    "print(\"The training accuracy is:\", get_accuracy(y_tr, x_tr, w))\n",
    "print(\"The validation accuracy is:\", get_accuracy(y_v, x_v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\ML\\ML_Project\\ML-Project1\\implementations.py:153: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "[-14.3354574    3.746899    -9.85490194  -2.18203777   1.8717472\n",
      "  -3.69593095   1.92974711  -3.79830212   4.16409375  -5.40733\n",
      "  -2.17174701  -6.16254143   5.56965983  -3.72428611   7.71290165\n",
      "  -0.09641564  -0.16425868   0.01821051  -0.04100076   0.11286233\n",
      "   0.07847962   0.1254175   -3.21048466  -5.31499881  -0.95674435\n",
      "  -0.85923784  -0.85916764  -3.78157198  -3.73185073  -3.73308813\n",
      "  -4.33143215]\n",
      "The training accuracy is: -28.027565\n",
      "The validation accuracy is: -27.96242\n"
     ]
    }
   ],
   "source": [
    "initial_w = generate_w(x_tr.shape)\n",
    "max_iters = 10\n",
    "gamma = 0.0001\n",
    "w, loss = reg_logistic_regression(y_tr, x_tr, 0.01, initial_w, max_iters, gamma)\n",
    "print(loss)\n",
    "print(w)\n",
    "print(\"The training accuracy is:\", get_accuracy(y_tr, x_tr, w))\n",
    "print(\"The validation accuracy is:\", get_accuracy(y_v, x_v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: 0.74388\n",
      "The validation accuracy is: 0.74368\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_tr, x_tr)\n",
    "print(\"The training accuracy is:\", get_accuracy(y_tr, x_tr, w))\n",
    "print(\"The validation accuracy is:\", get_accuracy(y_v, x_v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: 0.742425\n",
      "The validation accuracy is: 0.74196\n"
     ]
    }
   ],
   "source": [
    "w, loss = ridge_regression(y_tr, x_tr, 0.005)\n",
    "print(\"The training accuracy is:\", get_accuracy(y_tr, x_tr, w))\n",
    "print(\"The validation accuracy is:\", get_accuracy(y_v, x_v, w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6b41961f93c6f250bc55b15f7bfcab9769f15680d1f879ddd8dd4e686622cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
